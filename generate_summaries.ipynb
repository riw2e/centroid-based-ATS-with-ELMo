{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4dde8e1-9b8b-4fbd-9087-270a63b4ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, os, math, json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46f53c88-b426-454f-a457-3b52263d2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d11e360-b28a-4362-9977-0a6e6119ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "elmo = hub.load(\"../../tfhub/elmo3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e60de-7cb4-4bbc-a55e-f74568ad879b",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88ea7a9f-8aac-44c3-bb45-8c25cc8a7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opens the files\n",
    "def open_file(filename):\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bfdfb50-8758-422c-aa19-02bff982e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the text from the files into stories and highlights (reference summaries)\n",
    "def split_text(text):\n",
    "    stories = list()\n",
    "    index = text.find('@highlight')\n",
    "    doc, highlights = text[:index], text[index:].split('@highlight')\n",
    "    stories.append({'story':doc,'highlights':highlights})\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4cb951e-060d-4566-911e-99f34d5085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes the text to create sentence tokens, word tokens, and removes stop words\n",
    "def tokenize(text):\n",
    "    #we want to keep the processed and unprocessed text\n",
    "    processed = list()\n",
    "    unprocessed = list()\n",
    "    #tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        new_sent = list()\n",
    "        #tokenize the words in the sentences\n",
    "        unprocessed.append(word_tokenize(sentence))\n",
    "        #remove punctuation\n",
    "        tokens = word_tokenize(sentence.lower().translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
    "        #remove stop words from the sentences\n",
    "        filtered_sent = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        for w in filtered_sent:\n",
    "            new_sent.append(w)\n",
    "            #new_sent.append(ps.stem(w))\n",
    "            #print(w, \" : \", ps.stem(w))\n",
    "        processed.append(new_sent)\n",
    "\n",
    "    #return the processed text, the original text, and the tokenized sentences in the text\n",
    "    return processed, unprocessed, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a614ab-b752-4a8b-a6f6-b9f88722ff54",
   "metadata": {},
   "source": [
    "# Word frequency algorithm\n",
    "\n",
    "This algorithm is used to extract topic words from the articles. It accepts a text and returns a list of the topic words in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c52d98ac-8694-43ce-a144-85eed1d987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract topic words using word frequency\n",
    "def word_frequency(text):\n",
    "    vocabulary = {}\n",
    "    total_word_length = 0\n",
    "    for x in text:\n",
    "        for y in x:\n",
    "            total_word_length += 1\n",
    "            if y in vocabulary:\n",
    "                vocabulary[y] += 1\n",
    "            else:\n",
    "                vocabulary[y] = 1\n",
    "    #add only the top 10% of words to the list\n",
    "    highest = [(vocabulary[key], key) for key in vocabulary]\n",
    "    highest.sort()\n",
    "    highest.reverse()\n",
    "    total = len(highest)\n",
    "    top = total * 0.1\n",
    "    topic = list()\n",
    "    for x in range(int(top)):\n",
    "        topic.append(highest[x])\n",
    "        #print(highest[x])\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac261d-9308-4db7-9f01-bb4a24b67f91",
   "metadata": {},
   "source": [
    "# Centroid and sentence embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf953abd-1f43-486d-8ae2-c8f8fb7b87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes centroid embeddings from word2vec vectors\n",
    "def make_vec(words):\n",
    "    embedding = [1]\n",
    "    for x in range(len(words)):\n",
    "        if words[x][1] in wv:\n",
    "            embedding += wv[words[x][1]]\n",
    "    return embedding\n",
    "\n",
    "#makes the sentence vectors from word2vec embeddings\n",
    "def sent_vec(sentences):\n",
    "    embedding = list()\n",
    "    #for each sentence\n",
    "    for x in range(len(sentences)):\n",
    "        temp = [1]\n",
    "        #if the word is in word2vec, add to the embedding\n",
    "        for word in sentences[x]:\n",
    "            if word in wv:\n",
    "                temp += wv[word]\n",
    "        embedding.append(temp)\n",
    "    return embedding\n",
    "\n",
    "#make the centroid vectors from elmo embeddings\n",
    "def make_elmo_centroid(words):\n",
    "    temp = \"\"\n",
    "    for x in range(len(words)):\n",
    "        temp += words[x][1] + \" \"\n",
    "    embedding = elmo.signatures[\"default\"](tf.constant([temp]))[\"elmo\"]\n",
    "    return embedding\n",
    "\n",
    "#make the sentence vectors from elmo embeddings\n",
    "def make_elmo_vec(words):\n",
    "    embedding = elmo.signatures[\"default\"](tf.constant(words))[\"elmo\"]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8ee64-577f-4970-b9f9-90e2eb7d7751",
   "metadata": {},
   "source": [
    "# Cosine similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "80ad08a3-9987-474d-8cb1-5a8acf0bd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the cosine similarity of each sentence in a document and the centroid for each document\n",
    "def cos_sim(centroid,corpus):\n",
    "    cos_sim_sents = []\n",
    "    #for each doucment\n",
    "    for x in range(len(corpus)):\n",
    "        #arrays should be np\n",
    "        centroid_vec = np.array([centroid[x]])\n",
    "        sentences = []\n",
    "        #for each sentence in the document\n",
    "        for y in range(len(corpus[x])):\n",
    "            if len(corpus[x][y]) != 1:\n",
    "                sentence = np.array([corpus[x][y]])\n",
    "                #calculate the cosine similarity of the sentence and the centroid\n",
    "                sentences.append((cosine_similarity(centroid_vec,sentence).tolist(),y))\n",
    "            #sort sentences from high to low\n",
    "            sentences.sort(reverse=True)\n",
    "        cos_sim_sents.append(sentences)\n",
    "\n",
    "    return cos_sim_sents\n",
    "\n",
    "def elmo_cos_sim(centroid,corpus):\n",
    "    cos_sim_sents = []\n",
    "    for x in range(len(corpus)):\n",
    "        cur_centroid = 0\n",
    "        for y in range(len(centroid[x][0])):\n",
    "            cur_centroid = np.add(cur_centroid, centroid[x][0][y])\n",
    "        cur_centroid = np.array([cur_centroid])\n",
    "        sentences = []\n",
    "        for y in range(len(corpus[x])):\n",
    "            cur_sent = 0\n",
    "            \n",
    "            for z in range(len(corpus[x][y])):\n",
    "                cur_sent = np.add(cur_sent, corpus[x][y][z])\n",
    "            cur_sent = np.array([cur_sent])\n",
    "            \n",
    "            sentences.append((cosine_similarity(cur_centroid,cur_sent).tolist(),y))\n",
    "            sentences.sort(reverse=True)\n",
    "        cos_sim_sents.append(sentences)\n",
    "    return cos_sim_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad73912-a9dd-4ace-9ab3-8e9b20a12b89",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7d555bfc-d4f5-4426-a040-c030178d865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/'\n",
    "files = os.listdir(directory)\n",
    "stories = []\n",
    "for file in files:\n",
    "    filename = directory + '/' + file\n",
    "    text = open_file(filename)\n",
    "    stories.append(split_text(text))\n",
    "    \n",
    "#these hold the preprocessed and unprocessed summaries\n",
    "corpus_p = list()\n",
    "corpus_u = list()\n",
    "temp_stories = list()\n",
    "corpus_sentences = list()\n",
    "\n",
    "#for each document, preprocess the document by tokenizing the sentences and words and removing stop words\n",
    "for x in range(len(stories)):\n",
    "    temp_processed, temp_unprocessed, sentences = tokenize(stories[x][0]['story'])\n",
    "    corpus_p.append(temp_processed)\n",
    "    corpus_u.append(temp_unprocessed)\n",
    "    corpus_sentences.append(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a3b3c-9e71-4e02-9b3c-fbcb6e569957",
   "metadata": {},
   "source": [
    "# Topic word extraction\n",
    "\n",
    "Topic words for each article in the corpus are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4861a0d0-d465-42f3-bc9b-bd8f1b96b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the word frequency algorithm\n",
    "wf_topic = list()\n",
    "#get topic words for each document\n",
    "for x in range(len(corpus_p)):\n",
    "    wf_topic.append(word_frequency(corpus_p[x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0403b8e-95ad-4bf9-baf9-14e14869649e",
   "metadata": {},
   "source": [
    "# ELMo centroid and sentence embeddings\n",
    "1. create the centroid embeddings for each article using the topic words\n",
    "2. create the sentence embeddings for each article\n",
    "3. calculate the cosine similarity between the centroid embeddings and each sentence embedding in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "667a45b1-84ed-4402-b72e-013a2cafde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the centroid embeddings for each article\n",
    "elmo_centroid = list()\n",
    "for x in range(len(wf_topic)):\n",
    "    elmo_centroid.append(make_elmo_centroid(wf_topic[x]))\n",
    "    \n",
    "#create vectors for every sentence in each article\n",
    "elmo_sentences = list()\n",
    "for x in range(len(corpus_sentences)):\n",
    "    elmo_sentences.append(make_elmo_vec(corpus_sentences[x]))\n",
    "    \n",
    "#calculate the cosine similarity between the centroids and the sentences\n",
    "elmo_cosine = elmo_cos_sim(elmo_centroid,elmo_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dff0d0-eb9a-40d9-b4e5-3497b665b9b0",
   "metadata": {},
   "source": [
    "# Word2Vec centroid and sentence embeddings\n",
    "1. create the centroid embeddings for each article using the topic words\n",
    "2. create the sentence embeddings for each article\n",
    "3. calculate the cosine similarity between the centroid embeddings and each sentence embedding in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "28e0e230-012b-4e65-9bfe-9b063b5be3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the centroid embeddings for each article\n",
    "wv_centroid = list()\n",
    "for x in range(len(wf_topic)):\n",
    "    wv_centroid.append(make_vec(wf_topic[x]))\n",
    "    \n",
    "#create vectors for every sentence in each article\n",
    "wv_sentences = list()\n",
    "for x in range(len(corpus_p)):\n",
    "    wv_sentences.append(sent_vec(corpus_p[x]))\n",
    "    \n",
    "#calculate the cosine similarity between the setneces and the centroids\n",
    "wv_cosine = cos_sim(wv_centroid,wv_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe6ec0-a887-4335-8d41-fa44be3f5531",
   "metadata": {},
   "source": [
    "# Create summaries and write them to summaries.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d6e35812-3747-4702-a169-817f788f304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dict = {}\n",
    "for x in range(len(stories)):\n",
    "    tempdict = {}\n",
    "\n",
    "    #this tells us how many sentences to extract from each document, it needs to be the same number of\n",
    "    #sentences in the highlights because we are using ROUGE to compare them.\n",
    "    cur = len(stories[x][0]['highlights'])\n",
    "    \n",
    "    tempstr = \"\"\n",
    "    #add the reference summary to the json file\n",
    "    for y in range(1,cur):\n",
    "        tempstr += stories[x][0]['highlights'][y].strip('\\n') + \". \"\n",
    "    #store the reference summary in the temporary dictionary\n",
    "    tempdict['reference'] = tempstr\n",
    "    \n",
    "    #add the word2vec summary to the json file\n",
    "    tempstr = \"\"\n",
    "    for y in range(cur-1):\n",
    "        #the summary is created by using the index of the highest scoring sentences\n",
    "        #to get the correct sentences from the unprocessed article\n",
    "        tempstr += TreebankWordDetokenizer().detokenize(corpus_u[x][wv_cosine[x][y][1]]) + \" \"\n",
    "    #store the word2vec summary in the temporary dictionary\n",
    "    tempdict['word2vec'] = tempstr\n",
    "    \n",
    "    \n",
    "    #add the elmo summary to the json file\n",
    "    tempstr = \"\"\n",
    "    for y in range(cur-1):\n",
    "        #the summary is created by using the index of the highest scoring sentences\n",
    "        #to get the correct sentences from the unprocessed article\n",
    "        tempstr += TreebankWordDetokenizer().detokenize(corpus_u[x][elmo_cosine[x][y][1]]) + \" \"\n",
    "    #store the elmo summary in the temporary dictionary\n",
    "    tempdict['elmo'] = tempstr\n",
    "    \n",
    "    #store the temporary dictionary in the summary dictionary which will contain all summaries\n",
    "    sum_dict[str(x)] = tempdict\n",
    "    \n",
    "#write the summary dictionary to the json file\n",
    "with open(\"summaries.json\", \"w\") as outfile:\n",
    "    json.dump(sum_dict, outfile,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad23ee2-b55c-4201-9848-b86354a54462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7058adbc-a06f-470f-8f75-5a45102709ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code reopens the json file and stores the contents in arrays\n",
    "#it is redundant to do this, but oh well\n",
    "with open('summaries.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "references = []\n",
    "wv_summaries = []\n",
    "elmo_summaries = []\n",
    "for x in data:\n",
    "    references.append(data[x]['reference'].translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
    "    wv_summaries.append(data[x]['word2vec'].translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
    "    elmo_summaries.append(data[x]['elmo'].translate(str.maketrans(\"\",\"\",string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8a320-cc4a-446e-a882-367c4d419cea",
   "metadata": {},
   "source": [
    "# Calculate the average ROUGE scores for the Word2Vec and ELMo models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0bcdbfc8-da9d-41c1-84b8-856d6539b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a rouge class object\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c0967ef-3bb3-4f84-88ee-7bd94445f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will hold the rouge scores for the word2vec and elmo models\n",
    "wv_rouge1 = [0,0,0]\n",
    "wv_rouge2 = [0,0,0]\n",
    "wv_rougeL = [0,0,0]\n",
    "elmo_rouge1 = [0,0,0]\n",
    "elmo_rouge2 = [0,0,0]\n",
    "elmo_rougeL = [0,0,0]\n",
    "\n",
    "#sum the rouge scores for word2vec and elmo summaries\n",
    "for x in range(len(references)):\n",
    "    #rouge scores for word2vec\n",
    "    scores = rouge.get_scores(references[x],wv_summaries[x])\n",
    "    wv_rouge1[0] += scores[0]['rouge-1']['r']\n",
    "    wv_rouge1[1] += scores[0]['rouge-1']['p']\n",
    "    wv_rouge1[2] += scores[0]['rouge-1']['f']\n",
    "\n",
    "    wv_rouge2[0] += scores[0]['rouge-2']['r']\n",
    "    wv_rouge2[1] += scores[0]['rouge-2']['p']\n",
    "    wv_rouge2[2] += scores[0]['rouge-2']['f']\n",
    "\n",
    "    wv_rougeL[0] += scores[0]['rouge-l']['r']\n",
    "    wv_rougeL[1] += scores[0]['rouge-l']['p']\n",
    "    wv_rougeL[2] += scores[0]['rouge-l']['f']\n",
    "    \n",
    "    #rouge scores for elmo\n",
    "    scores = rouge.get_scores(references[x],elmo_summaries[x])\n",
    "    elmo_rouge1[0] += scores[0]['rouge-1']['r']\n",
    "    elmo_rouge1[1] += scores[0]['rouge-1']['p']\n",
    "    elmo_rouge1[2] += scores[0]['rouge-1']['f']\n",
    "\n",
    "    elmo_rouge2[0] += scores[0]['rouge-2']['r']\n",
    "    elmo_rouge2[1] += scores[0]['rouge-2']['p']\n",
    "    elmo_rouge2[2] += scores[0]['rouge-2']['f']\n",
    "\n",
    "    elmo_rougeL[0] += scores[0]['rouge-l']['r']\n",
    "    elmo_rougeL[1] += scores[0]['rouge-l']['p']\n",
    "    elmo_rougeL[2] += scores[0]['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "88381474-b358-4ece-8715-40851f638ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the average of the rouge scores for word2vec and elmo\n",
    "count = len(references)\n",
    "for x in range(3):\n",
    "    wv_rouge1[x] = wv_rouge1[x]/count\n",
    "    wv_rouge2[x] = wv_rouge2[x]/count\n",
    "    wv_rougeL[x] = wv_rougeL[x]/count\n",
    "\n",
    "    elmo_rouge1[x] = elmo_rouge1[x]/count\n",
    "    elmo_rouge2[x] = elmo_rouge2[x]/count\n",
    "    elmo_rougeL[x] = elmo_rougeL[x]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b5f649ce-7295-4cb4-a134-c164b6def76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\trecall\t\t     precision\t\t  f1\n",
      "ROUGE-1 w2v:\t [0.12645142914827953, 0.30833395268177877, 0.17748443126713617]\n",
      "ROUGE-1 elmo:\t [0.14357858136674712, 0.3283351913786697, 0.19896955416670864]\n",
      "\n",
      "ROUGE-2 w2v:\t [0.017095959595959595, 0.05228571428571429, 0.02543093416917817]\n",
      "ROUGE-2 elmo:\t [0.017872055104239545, 0.053000000000000005, 0.026640326531552093]\n",
      "\n",
      "ROUGE-L w2v:\t [0.06810666695509215, 0.1671869193608324, 0.09584485512889533]\n",
      "ROUGE-L elmo:\t [0.07769214415083574, 0.18131797349188652, 0.10835475056384565]\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\trecall\\t\\t     precision\\t\\t  f1')\n",
    "print('ROUGE-1 w2v:\\t', wv_rouge1)\n",
    "print('ROUGE-1 elmo:\\t', elmo_rouge1)\n",
    "print()\n",
    "print('ROUGE-2 w2v:\\t',wv_rouge2)\n",
    "print('ROUGE-2 elmo:\\t', elmo_rouge2)\n",
    "print()\n",
    "print('ROUGE-L w2v:\\t',wv_rougeL)\n",
    "print('ROUGE-L elmo:\\t',elmo_rougeL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
